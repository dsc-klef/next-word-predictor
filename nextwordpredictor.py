# -*- coding: utf-8 -*-
"""nextwordpredictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dpsRO2ZuVToF3j48EoGkT7QTiLkjMhXd
"""

import numpy as np 
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical 
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional

"""For the Shakespeare's Data:
https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154
"""

# sonnets.txt
!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K

#Reading Data
SONNETS_FILE = './sonnets.txt'

with open('./sonnets.txt') as f:
    data = f.read()

corpus = data.lower().split("\n")

print(f"The first 5 lines look like this:\n")
for i in range(5):
  print(corpus[i])

"""**Tokenization**"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1

corpus[0]

tokenizer.texts_to_sequences(corpus[0])

#sequence to list
tokenizer.texts_to_sequences([corpus[0]])[0]

"""**N-grams generation**"""

def n_gram_seqs(corpus, tokenizer):
  input_sequences = []
  
  for line in corpus:
      token_list = tokenizer.texts_to_sequences([line])[0]
      for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i + 1]
        input_sequences.append(n_gram_sequence)
      return input_sequences

input_sequences = n_gram_seqs(corpus, tokenizer)

max_sequence_len = max([len(x) for x in input_sequences])

print(f"n_grams of input_sequences have length: {len(input_sequences)}")
print(f"maximum length of sequences is: {max_sequence_len}")

"""**Padding sequences**"""

def pad_seqs(input_sequences, maxlen):
  padded_sequences = np.array(pad_sequences(input_sequences, maxlen = maxlen))
  return padded_sequences

input_sequences = pad_seqs(input_sequences, max_sequence_len)

print(f"padded corpus has shape: {input_sequences.shape}")

def features_and_labels(input_sequences, total_words):
  features = input_sequences[:, :-1]
  labels = input_sequences[:, -1]
  one_hot_labels = to_categorical(labels, num_classes = total_words)

  return features, one_hot_labels

features, labels = features_and_labels(input_sequences, total_words)

print(f"features have shape: {features.shape}")
print(f"labels have shape: {labels.shape}")

#Creating the model

def create_model(total_words, max_sequence_len):
  model = Sequential()

  model.add(Embedding(total_words, 100, input_length = max_sequence_len - 1))
  model.add(Bidirectional(LSTM(150)))
  model.add(Dense(total_words, activation = 'softmax'))

  model.compile(loss = 'categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = ['accuracy'])
  return model

"""**Training the Model**"""

model = create_model(total_words, max_sequence_len)

history = model.fit(features, labels, epochs=100, verbose=1)

seed_text = "Help me Obi Wan Kenobi, you're my only hope"
next_words = 95


for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	# probabilities of predicting a word
	predicted = model.predict(token_list, verbose=0)
	# Choose the next word based on the maximum probability
	predicted = np.argmax(predicted, axis=-1).item()
	output_word = tokenizer.index_word[predicted]
	seed_text += " " + output_word

print(seed_text)

#Observing the training curves of the model

acc = history.history['accuracy']
loss = history.history['loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training accuracy')
plt.title('Training accuracy')

plt.figure()

plt.plot(epochs, loss, 'b', label='Training Loss')
plt.title('Training loss')
plt.legend()

plt.show()